---
title: "Homework 3 - Group E"
author: "Michela Venturini, Rabindra Khadka, Gabriele Sarti"
date: "April 22, 2019"
output:
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
library(rstan); library(boot); library(bayesplot)
```

# Lectures

## Exercise 1

**Compute the bootstrap-based confidence interval for the `score` dataset using the studentized method.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}

```

## Exercise 2

**Compute bootstrap-based confidence intervals for the `score` dataset using the `boot` package.**

The `score` dataset was found [here](https://web.stanford.edu/~hastie/CASI_files/DATA/student_score.txt).

```{r echo=TRUE,  message=FALSE, warning=FALSE}
psi_fun <-function(data, id) {
  d <- data[id,]
  eig <- eigen(cor(d))$values
  return(max(eig)/ sum(eig))
}

score <-read.table("student_score.txt", header = TRUE)
psi_boot <- boot(data = score, statistic = psi_fun, R=1000)
boot.ci(boot.out = psi_boot, type = c("norm", "basic", "perc"))
```

# Laboratory

## Exercise 1

**Use `nlm` to compute the variance for the estimator $ \hat w = (\log(\hat \gamma),\log(\hat \beta))$ and `optimHess` for the variance of $\hat \theta=(\hat \gamma, \hat \beta)$.**

```{r echo=TRUE, message=FALSE, warning=FALSE}

```

## Exercise 2

**The Wald confidence interval with level $1 - \alpha$ is defined as:**

$$\hat \gamma \pm z_{1−\alpha/2}j_P(\hat \gamma)^{−1/2}$$

**Compute the Wald confidence interval of level 0.95 and plot the results.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}

```

## Exercise 3

**Repeat the steps above — write the profile log-likelihood, plot it and find the deviance confidence intervals — considering this time γ as a nuisance parameter and β as the parameter of interest.**

Since $\gamma$ is not expressed in function of $\beta$, we simply fix the value for $\gamma$ at $\hat \gamma$ and compute the profile log likelihood. Subsequently we proceed to compute the deviance confidence intervals with level $1-\alpha$ as:

$$\{\beta: \ell_P(\beta) \geq \ell_P(\hat \beta) - \frac{1}{2}\chi^2_{1;1-\alpha}\}$$

```{r echo=TRUE,  message=FALSE, warning=FALSE}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
 170.3, 148, 140, 118, 144, 97)
n <- length(y)

log_lik_weibull <- function(data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

weib.y.mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T, method='L-BFGS-B',lower=rep(1e-7,2),  upper=rep(Inf,2),data=y)

gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)

log_lik_weibull_profile <- function(data, beta) {
  gamma.beta <- uniroot(function(x) n/x+sum(log(data))-n*sum(data^x*log(data))/sum(data^x), c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}

log_lik_weibull_profile_v <-Vectorize(log_lik_weibull_profile, 'beta')

plot(function(x) -log_lik_weibull_profile_v(data=y, x) + weib.y.mle$value, from=120,to=200, xlab=expression(beta), ylab='profile relative log likelihood', ylim=c(-10,0))

conf.level<-0.95

abline(h=-qchisq(conf.level,1)/2, lty='dashed', col=2)

lrt.ci1 <- uniroot(function(x) -log_lik_weibull_profile_v(y, x) + weib.y.mle$value + qchisq(conf.level, 1)/2, c(1e-7, weib.y.mle$par[2]))$root

lrt.ci1 <- c(lrt.ci1,uniroot(function(x) -log_lik_weibull_profile_v(y,x) + weib.y.mle$value + qchisq(conf.level,1)/2, c(weib.y.mle$par[2],200))$root)

segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1], -log_lik_weibull_profile_v(y, lrt.ci1[1]), col="red", lty=2)

segments( lrt.ci1[2], -qchisq(conf.level,1)/2, lrt.ci1[2], -log_lik_weibull_profile_v(y, lrt.ci1[2]), col="red", lty=2)

points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)

points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)

segments(lrt.ci1[1], -10, lrt.ci1[2], -10, col="red", lty =1, lwd=2)

text(157, -9.5, "95% Deviance CI", col=2, cex=0.8)
```

## Exercise 4

**Perform a test as above, but with:**

$$\begin{cases}
H_0 : \gamma = 1 \\
H_1 : \gamma = 5
\end{cases}
$$

## Exercise 5

**We found that the posterior mean is a weighted mean of the prior belief and the likelihood mean. Using some simple algebra, retrieve other two alternative expression for $\mu^{*}$, completing the following, and provide a nice interpretation.**

$$1)\;\;\mu^*=\bar y - \dots \\
2)\;\;\mu^* = \mu + \dots
$$

## Exercise 6

**In `sim` in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of θ. Please, produce an histogram for these random draws $\theta^{(1)},\dots,\theta^{(S)}, compute the empirical quantiles, and overlap the true posterior distribution.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#input values

#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

y <- rnorm(n,theta_sample, sqrt(sigma2))

mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="normal.stan", data = data, chains = 4, iter=2000)
sim <- extract(fit)

# MCMC posterior estimate
hist(sim$theta, breaks=30, xlim=c(0.5,4), xlab= expression(theta), probability = TRUE, main = "")

# true posterior
curve(dnorm(x, mu_star, sd_star), col="red", add=TRUE, lwd=3)

# empirical quantiles
quant <- quantile(sim$theta)
segments(quant, 0, quant, dnorm(quant, mu_star, sd_star), col="blue", lwd=3)

legend(0.5,0.8, c("MCMC posterior", "True posterior", "Empirical quantiles"), c("black", "red", "blue"))
```

## Exercise 7

**Launch the following line of R code:**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
posterior <- as.array(fit)
```

**Use now the `bayesplot` package. Read the help and produce for this example, using the object posterior, the following plots:**

* **posterior intervals.**

* **posterior areas.**

* **marginal posterior distributions for the parameters.**

**Quickly comment.**

## Exercise 8

**Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim \text{Poisson}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:**

**1. $\pi(\lambda) = \text{Beta}(4,2)$**

**2. $\pi(\lambda) = \text{Normal}(1,2)$**

**3. $\pi(\lambda) = \text{Gamma}(4,2)$**

**Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.**

## Exercise 9

**Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the `rstan` library. Once you did it succesfully, open the file model called `biparametric.stan`, and replace the line:**

`target += cauchy_lpdf(sigma|0,2.5);`

**with the following one:**

`target += uniform_lpdf(sigma|0.1,10);`

**Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.**

We are assuming a uniform prior for parameter $\sigma$ such that $\sigma \sim \text{Unif}(0.1, 10)$.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
data<- list(N=n, y=y, a=-10, b=10)
fit <- stan(file="biparametric.stan", data = data, chains = 4, iter=2000, refresh=-1)
sim <- extract(fit)
posterior_biv <- as.matrix(fit)

theta_est <- mean(sim$theta)
sigma_est <- mean(sim$sigma)
c(theta_est, sigma_est)
traceplot(fit, pars=c("theta", "sigma"))
plot_title <- ggtitle("Posterior distributions", "with medians and 80% intervals")
mcmc_areas(posterior_biv, pars = c("theta","sigma"), prob = 0.8) + plot_title
```

We can see from the plots that mean and variance results for both $\theta$ and $\sigma$ are different, which is totally normal given the change we performed in our set of priors.

## Exercise 10

**Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a $\text{Gamma}(2,4)$. Then, compute the final Bayes factor matrix ($\text{BF_matrix}$) with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others?**

## Exercise 11

**Let $y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)$ collect the results of tossing $n=14$ times an unfair coin, where 1 denotes _heads_ and 0 _tails_, and $p=Prob(y_i=1)$.**

* **Looking at the `Stan` code for the other models, write a short Stan Beta-Binomial model, where $p$ has a $\text{Beta}(a,b)$ prior with $a=3, b=3$.**

* **extract the posterior distribution with the function `extract()`**

* **produce some plots with the `bayesplot` package and comment.**

* **compute analitically the posterior distribution and compare it with the `Stan` distribution.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
y <- c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <- 14
succ_prob <- sum(y)/n

alpha <- 3
beta <- 3

data <- list(N=n, y=sum(y), alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter = 2000)
sim <- extract(fit)

posterior_biv <- as.matrix(fit)
plot_title <- ggtitle("Posterior distributions", "with medians and 80% intervals")
mcmc_areas(posterior_biv, pars = c("p"), prob = 0.8) + plot_title

# prior distribution
curve(dbeta(x, alpha, beta), ylab = "density", lty = 2, col = "red", xlim = c(-0.25,1.25), ylim=c(0,4))

# Stan posterior distribution
lines(density(sim$p, adj=2), col="blue", lty=2, add=TRUE)

alpha_star <- alpha + sum(y)
beta_star <- beta + n - sum(y)

# true posterior distribution
curve(dbeta(x, alpha_star, beta_star), lty=1, col="black", add=TRUE)

legend(0.75, 3.5, c("Prior distribution", "Stan posterior", "True posterior"), c("red", "blue", "black"))
```

# Comment?