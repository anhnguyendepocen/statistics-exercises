---
title: "Homework 3 - Group E"
author: "Michela Venturini, Rabindra Khadka, Gabriele Sarti"
date: "May 6, 2019"
output:
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
library(rstan); library(boot); library(bayesplot)
library(ggplot2); library("MASS"); library(LearnBayes)
rstan_options(auto_write = TRUE)
```

# Lectures

## Exercise 1

**Compute the bootstrap-based confidence interval for the `score` dataset using the studentized method.**

The `score` dataset used for the exercise was found [here](https://web.stanford.edu/~hastie/CASI_files/DATA/student_score.txt).

```{r echo=TRUE,  message=FALSE, warning=FALSE}
score <- read.table("student_score.txt",header = TRUE)

# Parameter of interest PSI
# The parameter of interest is the eigenratio statistic for the correlation
# matrix of student_score: PSI = largest eigenvalue/sum eigenvalues:

psi_fun <- function(data) {
  eig <- eigen(cor(data))$values
  return(max(eig) / sum(eig))
}

# Observed value
psi_obs <- psi_fun(score)

# Compute confidence intervals
n <- length(score[,1]); B <- 10^4

s_aux <- rep(0,n)
s_vect <- rep(0, B)
SE_jack <- rep(0, B)

for(i in 1:B){
  ind <- sample(1:n, n, replace = TRUE)
  s_vect[i] <- psi_fun(score[ind,])
  
  for(j in 1:n) s_aux[j] <- psi_fun(score[ind,][-j,]) # Sample without j-th obs
  
  SE_jack[i] <- sqrt(((n - 1)/n) * sum((s_aux - mean(s_aux))^2))
}

SE_boot <- sd(s_vect)
psi_obs + c(-1, 1) * 1.96 * SE_boot

# Studentized bootstrap confidence interval
z<-(s_vect - psi_obs)/SE_jack
studentized_ci <- psi_obs - quantile(z, prob=c(0.975, 0.025))*SE_boot
studentized_ci

# Percentile method
perc_ci <- quantile(s_vect, prob=c(0.025, 0.975))
attr(perc_ci, "names") <- NULL
perc_ci

# Basic method
basic_ci <- 2 * psi_obs - quantile(s_vect, prob=c(0.975, 0.025))
attr(basic_ci, "names") <- NULL
basic_ci

Intervals<-c("Basic CI", "Perc CI", "Stud CI")
data <- data.frame(basic_ci, perc_ci, studentized_ci)
data <- data.frame(t(data), Intervals)

par(mfrow=c(1,2))
ggplot(data, aes(x=Intervals, color = Intervals)) +
  ggtitle("Plot of Confidence Intervals") +
  xlab("Confidence Intervals") + ylab(" ") +
  geom_errorbar( aes(x=data$Intervals, ymin=data$X2.5., ymax=data$X97.5.), width=0.4, alpha=0.9, size=1.3)+
  geom_abline(slope = 0, intercept = psi_obs, colour = "red", linetype = 2, size=1)+
  geom_text(aes(x = 2.5, y = .705, label = "PSI-observed"), colour = "red")

data <- data.frame(s_vect)
ggplot(data, aes(x=s_vect, y=..ncount..))+
  xlab("s_vect")+ ylab("Density") + 
  geom_histogram(binwidth = .005, fill="light grey") +
  geom_segment(aes(x = psi_obs , y = 1, xend = psi_obs, yend = 0), col=2) +
  geom_text(aes(x = .75, y = .9, label = "PSI-observed"), colour = "red") +
  geom_segment(aes(x = basic_ci[1] , y = 1, xend = basic_ci[1], yend = 0), col=3) +
  geom_segment(aes(x = basic_ci[2] , y = 1, xend = basic_ci[2], yend = 0), col=3)  +
  geom_text(aes(x = .4, y = .75, label = "Basic CI"), colour = 3) +
  geom_segment(aes(x = perc_ci[1] , y = 1, xend = perc_ci[1], yend = 0), col=4) +
  geom_segment(aes(x = perc_ci[2] , y = 1, xend = perc_ci[2], yend = 0), col=4) +
  geom_text(aes(x = .4, y = .50, label = "Percentile CI"), colour = 4) +
  geom_segment(aes(x = studentized_ci[1] , y = 1, xend = studentized_ci[1], yend = 0), col=5) +
  geom_segment(aes(x = studentized_ci[2] , y = 1, xend = studentized_ci[2], yend = 0), col=5) +
  geom_text(aes(x = .4, y = .25, label = "Studentized CI"), colour = 5)
```

## Exercise 2

**Compute bootstrap-based confidence intervals for the `score` dataset using the `boot` package.**

The `score` dataset used for the exercise was found [here](https://web.stanford.edu/~hastie/CASI_files/DATA/student_score.txt).

```{r echo=TRUE,  message=FALSE, warning=FALSE}
psi_fun_boot <-function(data, id) {
  d <- data[id,]
  eig <- eigen(cor(d))$values
  return(max(eig)/ sum(eig))
}

psi_boot <- boot(data = score, statistic = psi_fun_boot, R=10^4)
boot.ci(boot.out = psi_boot, type = c("basic", "perc"))

n <- length(score[,1])

psi_fun_boot_var <-function(data, id) {
  d <- data[id,]
  s_aux <- rep(0, n)
  SE_jack <- rep(0, 10^4)
  for (j in 1:n) s_aux[j] <- psi_fun(d[-j,])
  SE_jack[id] <- ((n - 1)/n) * sum((s_aux - mean(s_aux))^2)
  eig <- eigen(cor(d))$values
  out <- max(eig)/ sum(eig)
  return(c(out, SE_jack))
}

psi_boot_var <- boot(data = score, statistic = psi_fun_boot_var, R = 10^4)
boot.ci(psi_boot_var, type = "stud")
```

# Laboratory

## Exercise 1

**Use `nlm` to compute the variance for the estimator $\hat w = (\log(\hat \gamma),\log(\hat \beta))$ and `optimHess` for the variance of $\hat \theta=(\hat \gamma, \hat \beta)$.**

Having stored our data in `y`, `log_lik_weibull` is our log-likelihood function which takes two parameters, namely `data` and `param`. The latter is the name of the parameter vector, while the former is the name of the data object.

```{r echo=TRUE, message=FALSE, warning=FALSE}
#Sample.
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
 170.3, 148, 140, 118, 144, 97)
n <- length(y)

log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```

We perform a reparametrization using `theta` in order to express parameters estimates in log scale. Subsequently, we apply again the same reparametrization `theta` to return to the original values of the parameters.

```{r echo=TRUE, message=FALSE, warning=FALSE}
omega <- function(theta) log(theta)
theta <- function(omega) exp(omega)

log_lik_weibull_rep <- function(data, param) log_lik_weibull(data, theta(param))

weib.y.nlm<-nlm(log_lik_weibull_rep, c(0,0), hessian = TRUE, data = y)
weib.y.nlm
```

The above obtained `hessian` output is the observed Fisher information evaluated at the MLE,that is at $(\log(\hat \gamma),\log(\hat \beta))$. Now we use the `solve` function to obtain the variance-covariance matrix.

```{r echo=TRUE, message=FALSE, warning=FALSE}
diag(solve(weib.y.nlm$hessian))
```

The variance for the estimator $ \hat w = (\log(\hat \gamma),\log(\hat \beta))$ is `0.032473346` and `0.001582124` respectively.

Now by passing the reparametrized form of the parameters in the optimHess function; we can get the hessian evaluated at the MLE $(\hat \gamma, \hat \beta)$ 

```{r echo=TRUE, message=FALSE, warning=FALSE}
output.optimhess<-optimHess(theta(weib.y.nlm$estimate),log_lik_weibull,data=y)
output.optimhess 
```

We repeat the same procedure of above to obtain the variance-covariance matrix for the estimator $\hat \theta=(\hat \gamma, \hat \beta)$.

```{r echo=TRUE, message=FALSE, warning=FALSE}
diag(solve(output.optimhess))
```

The variance for the estimator  $\hat \theta=(\hat \gamma, \hat \beta)$ is `1.543241` and `38.406119` respectively.

## Exercise 2

**The Wald confidence interval with level $1 - \alpha$ is defined as:**

$$\hat \gamma \pm z_{1−\alpha/2}j_P(\hat \gamma)^{−1/2}$$

**Compute the Wald confidence interval of level 0.95 and plot the results.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#Define log-likelihood profile
log_lik_weibull_profile_gamma  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma) )
}

log_lik_weibull_profile_gamma_v <-Vectorize(log_lik_weibull_profile_gamma, 'gamma'  )

conf.level<-0.95

# MLE estimation using profile log likelihood with beta as nuisance parameter
weib.y.mle<-optim(1 ,fn=log_lik_weibull_profile_gamma,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

#Compute Wald CI
# 1. Compute SE by using the Hessian Matrix obtained by the MLE 
# estimation previously performed.
weib.y.se<-sqrt(diag(solve(weib.y.mle$hessian)))

# 2. Compute the two extremes of Wald CI.
wald.ci1<-weib.y.mle$par[1]+c(-1,1)*qnorm(1-(1-conf.level)/2)*weib.y.se[1]
wald.ci1

#Plot values for Profile ML with beta as nuisance parameter.
plot(function(x) -log_lik_weibull_profile_gamma_v(data=y, x)+weib.y.mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood',ylim=c(-8,0))

segments( wald.ci1[1], -log_lik_weibull_profile_gamma_v(y,wald.ci1[1])-weib.y.mle$value,
          wald.ci1[1], -log_lik_weibull_profile_gamma_v(y, wald.ci1[1])+weib.y.mle$value, 
          col="blue", lty=2)

segments( wald.ci1[2],
          -log_lik_weibull_profile_gamma_v(y,wald.ci1[2])-weib.y.mle$value, wald.ci1[2], -log_lik_weibull_profile_gamma_v(y, wald.ci1[2])+weib.y.mle$value, 
          col="blue", lty=2 )

segments(  wald.ci1[1], -7,  wald.ci1[2], -7, col="blue", lty =1, lwd=2)
points(wald.ci1[1], -qchisq(0.95,1)/2, pch=16, col="blue", cex=1.5)
points(wald.ci1[2], -qchisq(0.95,1)/2, pch=16, col="blue", cex=1.5)
abline(h=-qchisq(conf.level,1)/2,lty='dashed',col="blue")
abline(v=weib.y.mle$par[1], col=5, lwd=1, lty=2)
text(9.5, -1, "MLE for Gamma", col=5)
text(7,-6,"95% Wald CI",col="blue")

#Comparison with Deviance CI.
conf.level<-0.95
lrt.ci1<-uniroot(function(x) -log_lik_weibull_profile_gamma_v(y, x)+
                    weib.y.mle$value+
                    qchisq(conf.level,1)/2,
                  c(1e-7,weib.y.mle$par[1]))$root
 lrt.ci1<-c(lrt.ci1,uniroot(function(x) -log_lik_weibull_profile_gamma_v(y,x)+
                              weib.y.mle$value+
                              qchisq(conf.level,1)/2,
                            c(weib.y.mle$par[1],15))$root)
 segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1],
           -log_lik_weibull_profile_gamma_v(y, lrt.ci1[1]), col="red", lty=2  )
 segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
           -log_lik_weibull_profile_gamma_v(y, lrt.ci1[2]), col="red", lty=2  )
 segments( lrt.ci1[1],
           -8.1, lrt.ci1[2],
           -8.1, col="red", lty =1, lwd=2  )
 text(7,-7.5,"95% Deviance CI",col=2)
```

## Exercise 3

**Repeat the steps above — write the profile log-likelihood, plot it and find the deviance confidence intervals — considering this time γ as a nuisance parameter and β as the parameter of interest.**

Since $\gamma$ is not expressed in function of $\beta$, we simply fix the value for $\gamma$ at $\hat \gamma$ and compute the profile log likelihood. Subsequently we proceed to compute the deviance confidence intervals with level $1-\alpha$ as:

$$\{\beta: \ell_P(\beta) \geq \ell_P(\hat \beta) - \frac{1}{2}\chi^2_{1;1-\alpha}\}$$

```{r echo=TRUE,  message=FALSE, warning=FALSE}
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)

#Use optim to compute log likelihood for gamma and beta. 
#L-BFGS-B method uses a limited-memory modification of the 
#BFGS quasi-Newton method which allows box constraints, 
#that is each variable can be given a lower and/or upper bound. 
#The initial value must satisfy the constraints.
weib.y.mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)
#The first element is the MLE estimation for the shape(gamma) and 
#the second for the scale (beta):
weib.y.mle$par

log_lik_weibull_profile_beta <- function(data, beta) {
  gamma.beta <- uniroot(function(x) n/x - n * log(beta) + sum(log(data)) - sum((data/beta)^x * log(data/beta)), c(1e-5,15))$root
  log_lik_weibull(data, c(gamma.beta, beta))
}

log_lik_weibull_profile_beta_vec <-Vectorize(log_lik_weibull_profile_beta, 'beta')

plot(function(x) -log_lik_weibull_profile_beta_vec(data=y, x) + weib.y.mle$value, from=120,to=200, xlab=expression(beta),
     ylab='profile relative log likelihood', ylim=c(-10,0))

conf.level<-0.95
abline(h=-qchisq(conf.level,1)/2, lty='dashed', col=2)
lrt.ci1 <- uniroot(function(x) -log_lik_weibull_profile_beta_vec(y, x) + weib.y.mle$value 
                   + qchisq(conf.level, 1)/2, c(1e-7, weib.y.mle$par[2]))$root

lrt.ci1 <- c(lrt.ci1,uniroot(function(x) -log_lik_weibull_profile_beta_vec(y,x) + weib.y.mle$value 
                             + qchisq(conf.level,1)/2, c(weib.y.mle$par[2],200))$root)

segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1], -log_lik_weibull_profile_beta_vec(y, lrt.ci1[1]),
          col="red",lty=2)

segments( lrt.ci1[2], -qchisq(conf.level,1)/2, lrt.ci1[2], -log_lik_weibull_profile_beta_vec(y, lrt.ci1[2]),
          col="red", lty=2)

points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
segments(lrt.ci1[1], -10, lrt.ci1[2], -10, col="red", lty =1, lwd=2)
text(157, -9.5, "95% Deviance CI", col=2, cex=0.8)
```

## Exercise 4

**Perform a test as above, but with:**

$$\begin{cases}
H_0 : \gamma = 1 \\
H_1 : \gamma = 5
\end{cases}
$$

Given the above scenario, we are interested in testing  $H_0: \gamma = 1$; where $\gamma$ is the shape parameter of our weibull distribution. 

We first perform the likelihood ratio test: the hypothesis that  $\alpha = 1$ is equivalent to saying that data are distributed exponentially. The maximized log likelihood under the null hypothesis is given by following function.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
 log_lik_exp <- function(data, param){
   beta.gamma <- mean(data^param)^(1/param)
  -sum(dexp(data, 1/beta.gamma, log=TRUE))
}
 
#Maximized log likelihood under the null hypothesis.
l_0=log_lik_exp(y,1)
```

Similarly the maximized log likelihood under the alternative hypothesis is given by the profile likelihood function as below:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
log_lik_weibull_profile_gamma  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
 log_lik_weibull( data, c(gamma, beta.gamma) )
}

log_lik_weibull_profile_gamma_v <-Vectorize(log_lik_weibull_profile_gamma, 'gamma')

#Maximized log likelihood under the alternative hypothesis
 l_a=log_lik_weibull_profile_gamma(y,5)
 
```

Now the likelihood ratio test statistics is give by:

```{r echo=TRUE,  message=FALSE, warning=FALSE}

lambda_lrt <- -2*(l_a - l_0)
lambda_lrt
```

This likelihood ratio test follows $\chi^{2}$ distribution with 1 degree of freedom under the null hypothesis.

So,the p-value can be obtained as:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
p_lrt <- pchisq(lambda_lrt, df =1, lower.tail = FALSE)
p_lrt
```

From above result, we can observe that the associated p-value `p_lrt` is very small, so we reject $H_0$ at the 5% siginificance level. 

The Wald test statistic has the form  $W_e(\gamma_o)=\hat{\gamma}-\gamma_o/SE(\hat\gamma)^2$  which follows a Chi-square distribution with 1 degree of freedom. The Wald test statistics can be obtained as below:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#standard error
mle.se<-c(sqrt(diag(solve(output.optimhess))))

lambda_wald <- ((1/mle.se[1])^2)*(weib.y.mle$par[1]-1)^(2) 
lambda_wald
```

Now the assocaited p-value for the above Wald test is :

```{r echo=TRUE,  message=FALSE, warning=FALSE}
p_wald <- pchisq(lambda_wald, 1, lower.tail = FALSE)
p_wald
```
The associated p-value `p_wald` is also very small, so we reject the null hypothesis at 5% significance level.

The two tests performed above are asymtotically equivalent and both rejects the null hypothesis i.e. rejects the hypothesis that shape parameter is equal to 1.

## Exercise 5

**We found that the posterior mean is a weighted mean of the prior belief and the likelihood mean. Using some simple algebra, retrieve other two alternative expression for $\mu^{*}$, completing the following, and provide a nice interpretation.**

$$1)\;\;\mu^*=\bar y - \dots \\
2)\;\;\mu^* = \mu + \dots
$$

$$1.\;\; \mu^* = \overline{y}-\frac{\frac{1}{\tau^2}(\overline{y}-\mu)}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}$$.

From this formulation we can deduce that for $n \rightarrow \inf$ the Posterior mean $\mu^*$ coincides with the sample mean $\overline{y}$.

$$2. \;\; \mu^* = \mu + \frac{\frac{n}{\sigma^2}(\overline{y}-\mu)}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}} = \mu + \frac{(\overline{y}-\mu)}{1+\frac{\sigma^2}{\tau^2n}}$$

Also in this case we can deduce that for $n \rightarrow \inf$ the Posterior mean $\mu^*$ coincides with the sample mean $\overline{y}$.

From both representations, the Posterior mean can be seen as the prior mean/ MLE $\overline{y}$ with an adjustement depending on a weighted difference of the two. In particular, we can say that, as the size of the sample increases the prior mean $\mu^*$ loses importance in the determination of the Posterior mean $\mu^*$. We can also observe that if the variance of the prior $\tau^2$ is large the prior becomes uninformative and thus the prior mean has less influence on th e posterior.

## Exercise 6

**In `sim` in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of θ. Please, produce an histogram for these random draws $\theta^{(1)},\dots,\theta^{(S)}, compute the empirical quantiles, and overlap the true posterior distribution.**

The `normal.stan` file used for this exercise contains the following code:

```
data{
  int N;
  real y[N];
  real<lower=0> sigma;
  real mu;
  real<lower=0> tau;
}
parameters{
  real theta;
}
model{
  target+=normal_lpdf(y|theta, sigma);
  target+=normal_lpdf(theta|mu, tau );
}
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#input values

#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

y <- rnorm(n,theta_sample, sqrt(sigma2))

mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="normal.stan", data = data, chains = 4, iter=2000)
sim <- extract(fit)

# MCMC posterior estimate
hist(sim$theta, breaks=30, xlim=c(0.5,4), xlab= expression(theta), probability = TRUE, main = "")

# true posterior
curve(dnorm(x, mu_star, sd_star), col="red", add=TRUE, lwd=3)

# empirical quantiles
quant <- quantile(sim$theta)
segments(quant, 0, quant, dnorm(quant, mu_star, sd_star), col="blue", lwd=3)

legend(2.75,0.8, c("MCMC posterior", "True posterior", "Empirical quantiles"), c("black", "red", "blue"))
```

## Exercise 7

**Launch the following line of R code:**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
posterior <- as.array(fit)
```

**Use now the `bayesplot` package. Read the help and produce for this example, using the object posterior, the following plots:**

* **posterior intervals.**

* **posterior areas.**

* **marginal posterior distributions for the parameters.**

**Quickly comment.**

Here we use the `bayesplot` package that provides us with different plotting functions for visulaizing Markov chain Monte Carlo(MCMC) draws from the posterior distribution of the parameters of a Bayesian model.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#checking the dimension
dim(posterior)
dimnames(posterior)
```

Now by extracting posterior draws from the fitted model object, we use various functions from bayesplot package.

* **Plotting posterior intervals.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
color_scheme_set("red")
mcmc_intervals(posterior, pars = c("theta"))
```

Previous figure depicts the uncertainity interval of parameter theta computed from posterior draws with all chains merged. The circle point in the above figure is the posterior medians. The interval is based on the quantiles of the posterior distribution. By default the thick segments covers 50% intervals and the thinner outer lines covers 90% interval. So, given the data the 90% bayesian posterior interval has a probability of 0.9 of including the true `theta` parameter.

* **Plotting posterior area. **

```{r echo=TRUE,  message=FALSE, warning=FALSE}
mcmc_areas(
  posterior,
  pars = c("theta"),
  prob = 0.8, # 80% intervals,
  prob_outer = 0.9,
  point_est = "mean" )+
 ggplot2::labs(
   title = "Posterior distributions",
   subtitle = "with mean and 80% intervals"
 )
```

 Previous figure depicts the density plots computed from posterior draws with all chains merged. The shaded area under the curve is the uncertainity interval. The plot suggests that probability of `theta` being a member of 80% confidence interval is 0.8. The plot also shows the best estimate (mean) of theta around 2.0.


* **marginal posterior distributions for the parameters.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
color_scheme_set("green")
mcmc_hist(posterior, pars = c("theta"))
```

 Previous figure depicts the marginal posterior distribution for parameter of interest `theta`. The histogram indicates the number of times different values of `theta` were sampled from the posterior.

## Exercise 8

**Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $y_i \sim \text{Poisson}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:**

**1. $\pi(\lambda) = \text{Beta}(4,2)$**

**2. $\pi(\lambda) = \text{Normal}(1,2)$**

**3. $\pi(\lambda) = \text{Gamma}(4,2)$**

**Now, compute your posterior as $\pi(\lambda|y) \propto L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.**

The **Beta distribution** can be excluded immediately because it is defined in the interval $[0,1]$ and this problem concerns all the positive values since the posterior refers to the averege length of phone calls.

The **Normal distribution** can be excluded because it refers to the interval of all real numbers while we are interested in the positive numbers only.

The **Gamma distribution** seems to fit reasonably our problem and it is also the conjugate prior of the Poisson distribution so we can compute analitically the posterior. 

The **likelihood** is $$	L(\lambda|x)= \prod_{i=1}^{n}{\frac{e^{-\lambda}\lambda^{x_i}}{x_i!}}=\frac{e^{-\lambda}\lambda^{\sum{x_i}}}{\prod_{i=1}^{n}{x_i!}}$$

The **prior** is $$	p(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}$$

Then the **posterior** becomes $$\pi(\lambda|x) \propto \lambda^{\sum{x_i+\alpha-1}}e^{-(n+\beta)\lambda}$$
that is a Gamma distribution
$\pi(\lambda|y) = \text{Gamma}(\sum{y_i}+\alpha, n+\beta)$ with $\alpha=4$ , $n=15$ and $\beta=2$.
```{r echo=TRUE,  message=FALSE, warning=FALSE}
lambda.grid=seq(0,30,.1)
n=15;

y = rpois(n, lambda.grid);
lik <- function(lg,y){
  prod(dpois(y,lg))
}

#likelihood distribution
likel = sapply(lambda.grid, lik, y=y)/sum(sapply(lambda.grid, lik, y=y))
#prior distribution
prior = dgamma(lambda.grid, 4, rate=2)
## 1
posterior_true = (prior*likel)/sum(prior*likel)
## 2
alpha=sum(y)+4; beta=n+2
posterior_theoric = dgamma(lambda.grid,shape = alpha, rate = beta)
#plot distributions
corr = mean(posterior_theoric)/ mean(posterior_true)

plot(lambda.grid,prior, xlim= c(0,4), ylim=c(0,2), xlab = expression(lambda), 
     ylab = "Density", main = "Density Plot",col = "red", type = "l")
lines(lambda.grid,posterior_true*corr, col = "blue", lty=2,lwd=3)
lines(lambda.grid,posterior_theoric, col = "green")

legend("topright", col =c("red", "green", "blue" ), 
       c("Prior", "Theoretical Posterior", 
         "Actual Posterior"), lty=c(1,1,2),lwd=c(1,1,3), cex=1)

```

From the plot we can deduce that the theoretical posterior coincides with the posterior obtained as the product of likelihood and prior. 

## Exercise 9

**Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the `rstan` library. Once you did it succesfully, open the file model called `biparametric.stan`, and replace the line:**

`target += cauchy_lpdf(sigma|0,2.5);`

**with the following one:**

`target += uniform_lpdf(sigma|0.1,10);`

**Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.**

The `biparametric.stan` file used in this exercise contains the following code:

```
data{
  int N;
  real y[N];
  real a;
  real b;
}
parameters{
  real theta;
  real<lower=0> sigma;
}
model{
  target+=normal_lpdf(y|theta, sigma);
  target+=uniform_lpdf(theta|a, b );
  target+=uniform_lpdf(sigma| 0.1, 10);
}
```

We are assuming a uniform prior for parameter $\sigma$ such that $\sigma \sim \text{Unif}(0.1, 10)$.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
# Same variables from exercise 6
n <- 10
theta_sample <- 2
sigma2 <- 2
y <- rnorm(n,theta_sample, sqrt(sigma2))

data<- list(N=n, y=y, a=-10, b=10)
fit <- stan(file="biparametric.stan", data = data, chains = 4, iter=2000, refresh=-1)
sim <- extract(fit)
posterior_biv <- as.matrix(fit)

theta_est <- mean(sim$theta)
sigma_est <- mean(sim$sigma)
c(theta_est, sigma_est)
traceplot(fit, pars=c("theta", "sigma"))
plot_title <- ggtitle("Posterior distributions", "with medians and 80% intervals")
mcmc_areas(posterior_biv, pars = c("theta","sigma"), prob = 0.8) + plot_title
```

We can see from the plots that mean and variance results for both $\theta$ and $\sigma$ are slightly different, which is totally normal given the change we performed in our set of priors. The fact that the difference between the new plots and the old ones isn't evident and variances of parameters (especially theta's one) are quite large suggests that priors used don't provide much information to the posterior distributions.

## Exercise 10

**Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a $\text{Gamma}(2,4)$. Then, compute the final Bayes factor matrix ($\text{BF_matrix}$) with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others?**

We start by loading `soccergoals` data from `LearnBayes` library and reading in the data.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
data(soccergoals)
y <- soccergoals$goals
```

Given the assumptions that each soccer game lasts for same game time, the goals are rare independent events and with the same probability to occur throughout the game time, we consider Poisson distribution captures the distribution of number of goals for a given team.

$$\text{Goals} \sim \text{Poisson}(\lambda)$$

Since we are trying to estimate the parameter $\lambda$ in a Bayesian model,an initial set of plausible prior has to be provided. As we are counting number of goals which are always positive, we can take gamma distribution and lognormal distribution as a prior for the parameter $\lambda$.

So we first write the likelihood function using the gamma distribution.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}
```

Then we write the functions for the prior.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

prior_norm <- function(npar, theta){
  lambda <- exp(theta)  
  dnorm(theta, npar[1], npar[2])
}
 
# For efficiency ,changing the input format from a scalar to a vector

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")
```

Now, we construct the plot for the likelihood function and different priors with $\theta$ in x-axis and `density` in y axis. The variables `par` and `npar` are used to set different values of parameters for prior distribution.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
#likelihood
curve(lik_pois_v(theta=x, data=y), xlim=c(-1,4), xlab=expression(theta), ylab = "density", lwd =2 )

#prior 1
curve(prior_gamma_v(theta=x, par=c(2,4)), lty =2, col="red", add = TRUE, lwd =2)
 
#prior 2 
curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
 
#prior 3 
curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
 
#prior 4 
curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)

legend(2.6, 1.8, c("Lik.", "Ga(2,4)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)

```

The previous figure depicts the various priors and the likelihood function we are taking into account. We can observe that likelihood estimate of `theta` stands around 0.5. The red curve represents `prior 1` with gamma distribution which estimates mean in log scale around -0.48. The green curve for `prior three` is quite far from the likelihood and the blue-colored `prior two`. The violet curve represents `prior four` which is almost flat and is spread out with high variance. 

Then, using $\theta$=log($\lambda$) and calling above functions for likelihood and prior, we compute the log posteriors.

```{r echo=TRUE,  message=FALSE, warning=FALSE}

logpoissongamma <- function(theta, datapar){
   data <- datapar$data
   par <- datapar$par
   lambda <- exp(theta)
   log_lik <- log(lik_pois(data, theta))
   log_prior <- log(prior_gamma(par, theta))
   return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
 data <- datapar$data
 npar <- datapar$par
 lambda <- exp(theta)
 log_lik <- log(lik_pois(data, theta))
 log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

```

The log marginal likelihoods can be computed using the function laplace() as below:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
logmarg
```

We proceed to compare the above models to determine which ones are more likely and by how much by using Bayes Factor. This is the ratio between the marginal likelihood between two models. The larger the BF the better the model in the numerator position.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
   BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
   BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```
> BF Matrix computed after replacing `Prior 1` with a `Gamma(2,4)` 

**Conclusion:** As we can observe from the above BF matrix, `prior 2` still remains more favorable over other priors  after replacing Prior 1 with a Gamma(2,4) and matches our previous BF result computed in the lab exercise.

## Exercise 11

**Let $y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)$ collect the results of tossing $n=14$ times an unfair coin, where 1 denotes _heads_ and 0 _tails_, and $p=\text{Prob}(y_i=1)$.**

* **Looking at the `Stan` code for the other models, write a short Stan Beta-Binomial model, where $p$ has a $\text{Beta}(a,b)$ prior with $a=3, b=3$.**

* **extract the posterior distribution with the function `extract()`**

* **produce some plots with the `bayesplot` package and comment.**

* **compute analitically the posterior distribution and compare it with the `Stan` distribution.**

The beta-binomial stan file used is:

```
data{
  int N;
  int y;
  real<lower=0> alpha;
  real<lower=0> beta;
}

parameters{
  real theta;
}

model{
  target+=binomial_lpmf(y|N, theta); 
  target+=beta_lpdf(theta|alpha, beta);
}
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
y <- c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <- length(y)
N_1 <- sum(y) # Positive outcome estimate
N_0 <- n - N_1 # Negative outcome estimate

alpha <- 3
beta <- 3

data <- list(N=n, y=N_1, alpha=alpha, beta=beta)
fit <- stan(file="beta-binomial.stan", data = data, chains = 4, iter = 2000)
sim <- extract(fit)

posterior_biv <- as.matrix(fit)
plot_title <- ggtitle("Posterior distributions", "with medians and 80% intervals")
mcmc_areas(posterior_biv, pars = c("theta"), prob = 0.8) + plot_title
mcmc_intervals(posterior_biv, pars = c("theta"))
```

We can see above different visualizations generated using the `bayesplot` package of confidence intervals and posterior distribution for our beta-binomial model. We see from the plots that the variance of the parameter `p` isn't very large, that the parameter has its median around 0.35 and that the its 80% intervals goes roughly from 0.21 to 0.49.

The **likelihood** for the Beta-Binomial model is a binomial distribution such as:

$$p(Y|\theta) \sim \text{Bin}(N_1| \theta, N_0 + N_1)$$

and the **prior** is a beta distribution such as:

$$p(\theta) = \text{Beta}(\theta| \alpha, \beta)$$

Thus, we have that the true posterior is such that

$$p(\theta\;|\; Y) \propto \text{Bin}(N_1\;|\; \theta, N_0 + N_1) \;\;\text{Beta}(\theta\;|\; \alpha, \beta) \propto \text{Beta}(\theta\;|\; N_1 + \alpha, N_0 + \beta)$$

That is, a Beta distribution of parameters $N_1 + \alpha = 7, N_0 + \beta = 13$

```{r echo=TRUE,  message=FALSE, warning=FALSE}
# Prior distribution
curve(dbeta(x, alpha, beta), ylab = "density", lty = 2, col = "red", xlim = c(-0.25,1.25), ylim=c(0,4))
  
# Stan posterior distribution
lines(density(sim$theta, adj=2), col="blue", lty=2, add=TRUE)

# True posterior distribution
curve(dbeta(x, alpha + N_1, beta + N_0), lty=1, col="black", add=TRUE)

legend(0.75, 3.5, c("Prior distribution", "Stan posterior", "True posterior"), c("red", "blue", "black"))
```

We can see from the previous image that the Stan-generated posterior is very close to the true posterior for the Beta-Binomial distribution.