---
title: "Homework 2 - Group G"
author: "Marco Franzon, Eduardo Gonnelli, Gabriele Sarti"
date: "April 16, 2019"
output:
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
require(RColorBrewer); library(zoo); library(lattice)
```

# Core Statistics

## Exercise 3.3

**Rewrite the following, replacing the loop with efficient code**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1
for (i in 1:n) {
    if (z[i]<0) {
        zneg[j] <- z[i]
        j <- j + 1
    }
}
```
**Confirm that your rewrite is faster but gives the same result.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
set.seed(42)

start <- proc.time()
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1
for (i in 1:n) {
    if (z[i]<0) {
        zneg[j] <- z[i]
        j <- j + 1
    }
}
time <- proc.time() - start
print("Base loop timing:")
time

set.seed(42)

start <- proc.time()
n <- seq(100000)
z <- rnorm(n)
zneg_opt <- z[z < 0]
time <- proc.time() - start
print("Efficient rewrite of the loop timing:")
time

print("Are results equal?")
all(zneg_opt == zneg)
```

## Exercise 3.5

**Consider solving the matrix equation $Ax=y$ for $x$, where $y$ is a known $n$ vector and $A$ is a known $n×n$ matrix. The formal solution to the problem is $x=A^{−1}y$, but it is possible to solve the equation directly, without actually forming $A^{−1}$. This question explores this direct solution. Read the help file for solve before trying it**

**a. First create an $A,x$ and $y$ satisfying $Ax=y$.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
n <- 1000
A <- matrix(runif(n*n),n,n); x.true <- runif(n)
y <- A%*%x.true
```

**The idea is to experiment with solving $Ax=y$ for $x$, but with a known truth to compare the answer to.**

**b. Using solve, form the matrix $A^{−1}$ explicitly and then form $x_1=A^{−1}y$. Note how long this takes. Also assess the mean absolute difference between `x1` and `x.true`(the approximate mean absolute ‘error’ in the solution).**

As mentioned in the `solve` documentation, if the second parameter of solve is missing, it is taken to be an identity matrix and solve will return the inverse of the matrix passed as first parameter.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
start <- proc.time()
A.inv <- solve(A)
x1 <- A.inv%*%y
proc.time() - start
mean(abs(x1 - x.true))
```

**c. Now use `solve` to directly solve for $x$ without forming $A^{−1}$. Note how long this takes and assess the mean absolute error of the result.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
start <- proc.time()
x2 <- solve(A,y)
proc.time() - start
mean(abs(x2 - x.true))
```

**d. What do you conclude?**

The direct solution obtained in point c is not only way faster that the explicit one computed in point b, but also more precise by roughly two orders of magnitude ($e-11$ vs $e-13$)

# Data Analysis and Graphics Using R

## Exercise 3.10

**This  exercise  investigates  simulation  from  other  distributions.  The  statement `x<-rchisq(10, 1)` generates 10 random values from a chi-squared distribution with one degree of freedom. The statement `x <- rt(10, 1)` generates 10 random values from at-distribution with one degree of freedom. Make normal probability plots for samples of various sizes from each of these distributions. How large a sample is necessary, in each instance, to obtain a consistent shape?**

```{r echo=TRUE, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
for (n in c(10, 20, 50, 100, 250, 500)) {
  qqnorm(rchisq(n,1), ylim= c(-4,5),main = paste("chisq with n = ", n))
  qqline(rchisq(n,1), col=4)
  qqnorm(rt(n,1), ylim= c(-10,10), main = paste("t-student with n = ", n))
  qqline(rt(n,1), col=4)
}
```

In both case the shape becomes roughly consistent with 250 samples.

## Exercise 3.11

**The following data represent the total number of aberrant crypt foci (abnormal growths in the colon) observed in seven rats that had been administered a single dose of the carcinogen azoxymethane and sacrificed after six weeks (thanks to Ranjana Bird, Faculty of Human Ecology,University of Manitoba for the use of these data):**

`87 53 72 90 78 85 83`

**Enter these data and compute their sample mean and variance. Is the Poisson model appropriate for these data? To investigate how the sample variance and sample mean differ under the Poisson assumption, repeat the following simulation experiment several times:**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
x <- rpois(7, 78.3)
mean(x); var(x)
```

```{r echo=TRUE,  message=FALSE, warning=FALSE}
y <- c(87, 53, 72, 90, 78, 85, 83)
c(mean=mean(y), var=var(y))

R <- 10000
n <- 7
lambda <- 78.3

y <- matrix(rpois(n*R, lambda), R, n)
means <- apply(y, 1, mean)
vars <- apply(y, 1, var)

hist(vars, breaks=50, main = paste("Poisson's variances histogram with lambda = ", lambda))
hist(means, breaks=50, main = paste("Poisson's means histogram with lambda = ", lambda))
```

It could be noticed that the average Poisson variance is approximately 75, while sample variance is more than twice that value, at 159.90. We can take this result as an indication that the specified Poisson model is probably inappropriate to model these data.

## Exercise 3.13

**Markov chain for the weather in a particular season of the year has the transition matrix, from one day to the next:**

$$Pb = 
\begin{bmatrix}
        & Sun & Cloud & Rain \\
  Sun   & 0.6 & 0.2   & 0.2  \\
  Cloud & 0.2 & 0.4   & 0.4  \\
  Rain  & 0.4 & 0.3   & 0.3  \\
\end{bmatrix}
$$

**It can be shown, using linear algebra, that in the long run this Markov chain will visit the states according to the stationary distribution:**

$$Sun = 0.641, \;Cloud  = 0.208,\; Rain = 0.151$$
**A result called the ergodic theorem allows us to estimate this distribution by simulating the Markov chain for a long enough time.**

**(a)  Simulate 1000 values, and calculate the proportion of times the chain visits each of the states. Compare the proportions given by the simulation with the above theoretical proportions.**

The stationary distribution is wrong according to DAAG's errata corrige. Real proportions are:

$$Sun = 0.428, \;Cloud  = 0.286,\; Rain = 0.286$$

We use the Markov function given in DAAG exercise 3.12:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
Markov <- function (N=100, initial.value=1, P){
  X <- numeric(N)
  X[1] <- initial.value + 1  # States 0:5; subscripts 1:6
  n <- nrow(P)
  for (i in 2:N){
    X[i] <- sample(1:n, size=1, prob=P[X[i-1], ])
  }
  X-1
}

Pb <- matrix(nrow = 3, ncol = 3, byrow = TRUE, data = c(.6,.2,.2,.2,.4,.4,.4,.3,.3),
             dimnames = list(c("Sun", "Cloud", "Rain"), c("Sun", "Cloud", "Rain")))
chain <- factor(Markov(1000, 0, Pb), labels = c("Sun", "Cloud", "Rain"))
table(chain)/length(chain)
```

We see that simulation results are similar to theoretical proportions. By increasing the simulation size, the result gets approximated even better.

```{r echo=TRUE,  message=FALSE, warning=FALSE}
chain <- factor(Markov(10000, 0, Pb), labels = c("Sun", "Cloud", "Rain"))
table(chain)/length(chain)
```

**(b)  Here is code that calculates rolling averages of the proportions over a number of simulations and plots the result. It uses the function `rollmean()` from the zoo package.**

**Try varying the number of simulations and the width of the window. How wide a window is needed to get a good sense of the stationary distribution? This series settles down rather quickly to its stationary distribution (it “burns in” quite quickly). A reasonable width of window is, however, needed to give an accurate indication of the stationary distribution.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
plotmarkov <- function(n=10000, start=0, window=100, transition=Pb, npanels=5, title){
  xc2 <- Markov(n, start, transition)
  mav0 <- rollmean(as.integer(xc2==0), window)
  mav1 <- rollmean(as.integer(xc2==0), window)
  npanel <- cut(1:length(mav0), breaks=seq(from=1, to=length(mav0),
                length=npanels+1), include.lowest=TRUE)
  df <- data.frame(av0=mav0, av1=mav1, x=1:length(mav0), gp=npanel)
  print(xyplot(av0+av1 ~ x | gp, data=df, layout=c(1,npanels),
        type="l", par.strip.text=list(cex=0.65),
        scales=list(x=list(relation="free")),main = title))
}

for(n in c(10000,25000,50000))
  for(ww in c(100,1000,5000)) 
    plotmarkov(n = n, window = ww, title = paste0(n," simulations with window width = ",ww))
```

The wider the window, the better to get a good sense of the stationary distribution. This said, even a window in the order to 1000 is enough to show how the series starts to burn in.

## Exercise 4.6

**Here we generate random normal numbers with a sequential dependence structure:**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
y1 <- rnorm(51)
y <- y1[-1] + y1[-51]
acf(y1)     # acf is  ‘autocorrelation function’# (see Chapter 9)
acf(y)
```

**Repeat this several times. There should be no consistent pattern in the acf plot for different random samples `y1`. There will be a fairly consistent pattern in the acf plot for `y`,a result of the correlation that is introduced by adding to each value the next value in the sequence.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
for(i in seq(1:5)){
  y1 <- rnorm(51)
  acf(y1)
  y <- y1[-1] + y1[-51]
  acf(y)
}
```

The dependency introduced when building `y` is evident from the plots, since all values are fairly close to the previous ones, while the same is not true for values in `y1`.

## Exercise 4.7

**Create a function that does the calculations in the first two lines of the previous exercise. Put the calculation in a loop that repeats 25 times. Calculate the mean and variance for each vector `y` that is returned. Store the 25 means in the vector `av`, and store the 25 variances inthe vector `v`. Calculate the variance of `av`.**

``` {r echo=TRUE,  message=FALSE, warning=FALSE}
rnd_dep <- function(n = 51) {
  y1 <- rnorm(n)
  y <- y1[-1] + y1[-n]
}

av <- numeric(25)
v <- numeric(25)

for (i in 1:25) {
  y <- rnd_dep()
  av[i] <- mean(y)
  v[i] <- var(y)
}

var(av)
``` 

# Laboratory

## Exercise 1

**Check the biased nature of $s^2_b$ via MC simulation, generating $n=10$ iid values from a normal distribution. Plot also $s^2$ and comment the difference.**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
set.seed(42)

plot_var_estimators <- function(n, R, sigma){
  m <- matrix(data=rnorm(n*R, 0, sigma), nrow=n, ncol=R)
  stat <- array(0, R)
  stat <- apply(m, 2, var) # Sample variance
  hist(stat, breaks = 40, probability = TRUE, xlab=expression(s^2), main= bquote(s^2), cex.main=1.5)
  curve(((n-1)/sigma^2) * dchisq(x*((n-1)/sigma^2), df=n-1), add=TRUE, col="red", lwd=3) # Unbiased var estimator
  curve(((n)/sigma^2) * dchisq(x*((n)/sigma^2), df=n-1), add=TRUE, col="blue", lwd=3) # Biased var estimator
}

plot_var_estimators(5, 1000, 1)
plot_var_estimators(10, 1000, 1)
plot_var_estimators(200, 1000, 1)

```

We can see how the simulation size has a big impact on the biased estimator $s^2_b$. By growing the simulation size as done in the previous images, we see that the biased estimator $s^2_b$ tends to the values of the unbiased one $s^2$ since the difference between $n$ and $n-1$ in proportion to $n$ tends to vanish.

## Exercise 2

**What happens if a great player decides to join you, now? Try to simulate the data and perform the test again.**

Original test:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
set.seed(2)
n <- 50
K <- 4
M <- 6
y <- matrix(0, M, n )
# generate the values
for (m in 1:M){
  y[m, ] <- sample(1:K, n, replace=TRUE, prob = c( 7/16, 5/16, 3/16, 1/16) )
} 
observed_matrix <- apply(y,1, table)
chisq.test(observed_matrix, p = c( 7/16, 5/16, 3/16, 1/16))
```

New test with pro player:

```{r echo=TRUE}
set.seed(2)
n <- 50
K <- 4
M <- 7
y <- matrix(0, M, n)

# Generate the values
for (m in 1:M-1){
  y[m, ] <- sample(1:K, n, replace=TRUE, prob = c( 7/16, 5/16, 3/16, 1/16)) # Standard players
} 
y[M, ] <- sample(1:K, n, replace=TRUE, prob = c(1/16, 3/16, 4/16, 8/16)) # Pro player
observed_matrix <- apply(y,1, table)
chisq.test(observed_matrix, p = c( 7/16, 5/16, 3/16, 1/16))
```

The p-value correctly indicates that our performance uniformity hypothesis is misplaced in presence of a pro player.

## Exercise 3

**Sometimes it could be useful to assess the degree of association, or correlation, between paired samples, using the Pearson, the Kendall’s $\tau$ or the Spearman’s $\rho$ correlation coefficient. Regardless of the adopted cofficient, the null hypothesis for a given correlation coefficent $\rho$ is:**

$$H_0: \rho = 0$$

**The test statistic is then defined as:**

$$T = r \sqrt{\frac{n- 2}{1 - r^2}}\sim_{H_0} t_{n-2},$$

**where $r = Corr(X,Y)$ is the Pearson correlation coefficient. Suppose to have two samples of the same length $x_1, \dots ,x_n,\;y_1,\dots,y_n$, and to measure the association between them. Once we compute the test statistic $t_{obs}$, we may then compute the $p$-value (here we are evaluating a two sided test) as:**

$$p = 2Pr_{H_0}(T \geq |t_{obs}|)$$

**Consider now some of the most followed Instagram accounts in 2018: for each of the owners, we report also the number of Twitter followers (in milions). Are the Instagram and Twitter account somehow associated? Perform a correlation test, compute the p-value and give an answer. Here is the dataframe.**


```{r echo=TRUE,  message=FALSE, warning=FALSE}
Owners <- c( "Katy Perry", "Justin Bieber", "Taylor Swift", "Cristiano Ronaldo", "Kim Kardashian", "Ariana Grande", "Selena Gomez", "Demi Lovato")
Instagram <- c( 69, 98,107, 123, 110, 118, 135, 67)
Twitter <- c( 109, 106, 86, 72, 59, 57, 56, 56)
plot( Instagram, Twitter, pch=21, bg=2, xlim=c(60, 150), ylim=c(40, 120) )
text( Instagram[-6], Twitter[-6]+5, Owners[-6], cex=0.8 )
text( Instagram[6], Twitter[6]-5, Owners[6], cex=0.8 )

test <- cor.test(Instagram, Twitter, method = "pearson") 

plotclr <- brewer.pal(6,"YlOrRd")
curve(dt(x,6),xlim=c(-5,5), ylim=c(0,0.4),
  main="p-values and rejection region", col = "blue", lwd = 2, xlab="x-y",  ylab=expression(test[6]),  yaxs="i")
cord.x <- c(qt(0.95,6),seq(qt(0.95,6), 5, 0.01), 5)
cord.y <- c(0,dt(seq(qt(0.95,6), 5, 0.01),6),0)
polygon(cord.x,cord.y,col=plotclr[3], border = NA )
curve(dt(x,6),xlim=c(-5,5),main=expression(test[6]), col = "blue", lwd = 2, add = TRUE, yaxs="i")
abline(v =test$statistic, lty=2, lwd=2, col="red")
text (0,0.2, paste("Accept", expression(H0)))
text (2.7,0.08, paste("Reject", expression(H0)))
text(as.double(test$statistic)-0.15, 0.02, expression("t"), col="red", cex=1.2)
```

We see from the results of the Pearson test that the $p$-value is $0.29$, much greater than $\alpha = 0.05$. By plotting our results, it is evident that we should accept our $H_0$ of independence between followers on Twitter and Instagram. Thus, we may say that there is evidence that those accounts are not associated.

## Exercise 4

**Compute analitically** $J(γ,γ;y),J(γ,β;y),J(β,β;y)$.

We start from the log likelihood of Weibull distribution:

$$\ell(\gamma;\beta,y) = n\log\gamma - n\gamma\log\beta + \gamma\sum_{i=1}^n\log(y_i) - \sum_{i=1}^n(\frac{y_i}{\beta})^\gamma$$

We first take our first order partial derivatives with respect to $\beta$ and $\gamma$:

$$\frac{\partial\ell}{\partial\beta} = -\frac{n\gamma}{\beta} + \gamma\sum_{i=1}^n\frac{y_i^\gamma}{\beta^{\gamma+1}}$$

$$\frac{\partial\ell}{\partial\gamma} = \frac{n}{\gamma} - n\log\beta + \sum_{i=1}^n\log y_i - \sum_{i=1}^n (\frac{y_i}{\beta})^\gamma\log(\frac{y_i}{\beta})$$

Then, we compute all the second partial derivatives, respectively $\frac{\partial^2\ell}{\partial\beta^2}, \frac{\partial^2\ell}{\partial\gamma^2}$ and $\frac{\partial^2\ell}{\partial\beta\partial\gamma}$. Notice that this last term is the same independently from differentiation order thanks to the symmetry of Hessian matrix.

$$\frac{\partial^2\ell}{\partial\beta^2} = \frac{n\gamma}{\beta^2} - \gamma(\gamma + 1)\sum_{i=1}^n\frac{y_i^\gamma}{\beta^{\gamma+2}}$$

$$\frac{\partial^2\ell}{\partial\gamma^2} = -\frac{n}{\gamma^2} - \sum_{i=1}^n (\frac{y_i}{\beta})^\gamma(\log(\frac{y_i}{\beta}))^2$$

$$\begin{align}
\frac{\partial^2\ell}{\partial\beta\partial\gamma} = \frac{\partial^2\ell}{\partial\gamma\partial\beta} &= - \frac{n}{\beta} - \sum_{i=1}^n \frac{(y_i)^\gamma(-\gamma)}{\beta^{\gamma+1}})\log(\frac{y_i}{\beta}) + (\frac{y_i}{\beta})^\gamma(-\frac{1}{\beta}) \\
&= - \frac{n}{\beta} + \sum_{i=1}^n \frac{y_i^\gamma}{\beta^{\gamma+1}}(\gamma\log(\frac{y_i}{\beta}) + 1)
\end{align}
$$

The observed information matrix has the opposite of the two second-order partial derivatives on the main diagonal and the opposite of the symmetric second-order mixed derivatives on the sides, which are:

$$J(γ,γ;y) = \frac{n}{\gamma^2} + \sum_{i=1}^n (\frac{y_i}{\beta})^\gamma(\log(\frac{y_i}{\beta}))^2$$
$$J(γ,β;y) = \frac{n}{\beta} - \sum_{i=1}^n \frac{y_i^\gamma}{\beta^{\gamma+1}}(\gamma\log(\frac{y_i}{\beta}) + 1)$$
$$J(β,β;y) = - \frac{n\gamma}{\beta^2} + \gamma(\gamma + 1)\sum_{i=1}^n\frac{y_i^\gamma}{\beta^{\gamma+2}}$$

## Exercise 5

**Produce the contour plot for the quadratic approximation of the log-likelihood, based on the Taylor series:**

$$\ell(\theta) - \ell(\hat \theta) \simeq -\frac{1}{2}(\theta - \hat\theta)^T J(\hat \theta)(\theta - \hat \theta)$$

```{r echo=TRUE,  message=FALSE, warning=FALSE}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146, 170.3, 148, 140, 118, 144, 97)
n <- length(y)

# Define parameters grid
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)
parvalues <- expand.grid(gamma,beta)

gammahat <- uniroot(function(x) n / x + sum(log(y)) - n *
                  sum(y^x * log(y)) / sum(y^x),
                  c(1e-5, 15))$root 
betahat <- mean(y^gammahat)^(1 / gammahat)
weib.y.mle <- c(gammahat, betahat)

#observed information matrix
jhat <- matrix(NA, nrow=2, ncol=2)
jhat[1,1] <- n / gammahat^2 + sum((y / betahat)^gammahat * (log(y / betahat))^2)
jhat[1,2]<- jhat[2,1] <-  n / betahat - sum(y^gammahat / betahat^(gammahat + 1) * (gammahat * log(y / betahat) + 1))
jhat[2,2] <- - n * gammahat / betahat^2 + gammahat * (gammahat + 1) /
 betahat^(gammahat + 2) * sum(y^gammahat)

# Quadratic approximation of log-likelihood function
approximation <- function(theta){
  diff <- theta - weib.y.mle
  return(-0.5*diff%*%jhat%*%diff)
}

approx <- apply(parvalues, 1, approximation)
approx <- matrix(approx, nrow=length(gamma), ncol=length(beta), byrow=F )

# Contour plot

conf.levels <- c(0, 0.5, 0.75, 0.9, 0.95, 0.99)
contour(gamma, beta, approx,
        levels=-qchisq(conf.levels, 2) / 2,
        xlab=expression(gamma),
        labels=as.character(conf.levels),
        ylab=expression(beta))
title('Weibull of approximated log likelihood')

# Image plot

 image(gamma, beta, approx, zlim=c(-6,0),
 col=terrain.colors(20),xlab=expression(gamma),
 ylab=expression(beta))
 title('Weibull of approximated log likelihood')
```