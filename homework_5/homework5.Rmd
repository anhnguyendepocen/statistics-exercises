---
title: "Homework 5 - Sarti"
author: "Gabriele Sarti"
date: "June 23, 2019"
output:
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
library(DAAG); library(ggplot2)
```

# Data Analysis and Graphics Using R

## Exercise 4.21

**Suppose the mean reaction time to a particular stimulus has been estimated in several previous studies, and it appears to be approximately normally distributed with mean $0.35$ seconds with standard deviation $0.1$ seconds. On the basis of 10 new observations, the mean reaction time is estimated to be $0.45$ seconds with an estimated standard deviation of $0.15$ seconds. Based onthe sample information, what is the maximum likelihood estimator for the true mean reaction time? What is the Bayes’ estimate of the mean reaction time?**

Since the data are approximately normally distributed, the maximum likelihood estimators for the true mean reaction time are equal to the sample mean, $0.45$, and the sample variance $0.15^2 = 0.0225$.

In order to obtain the Bayes' estimate of the mean reaction time we use the formulas for the posterior density of the mean presented in chapter 4.8.2, which is normal with mean

$$\frac{n \bar y + \mu_0 \sigma^2 / \sigma_0^2}{n + \sigma^2 / \sigma_0^2}$$
and variance

$$\frac{\sigma^2}{n + \sigma^2 / \sigma_0^2}$$

We substitute the variables in the formula with the values given by the assignment:

* $n$ is the added sample size, $10$

* $\mu_0$ is the estimated mean of the original sample, $0.35$

* $\sigma_0$ is the standard deviation of the original sample, $0.1$

* $\bar y$ is the new sample mean, $0.45$

* We don't have the true standard deviation $\sigma$, but we can estimate it using the new sample standard deviation, $0.15$

We proceed to compute Bayes' estimates for both the mean and the variance of the distribution by computing the posterior mean and variance:

```{r echo=TRUE, message=FALSE, warning=FALSE}
n = 10
mu0 = 0.35
s0 = 0.1
y = 0.45
s = 0.15

print(paste("Bayes mean:", (n * y + mu0 * s^2 / s0^2)/(n + s^2 / s0^2)))
print(paste("Bayes variance:", (s^2)/(n + s^2 / s0^2)))
```

## Exercise 7.3

**Use the method of Section 7.3 to compare, formally, the regression lines for the two data frames `elastic1` and `elastic2` from Exercise 1 in Chapter 5.**

We start by creating single dataframe from `elastic1` and `elastic2`:

```{r echo=TRUE, message=FALSE, warning=FALSE}
elastic1$experiment <- rep(1, length(elastic1$stretch))
elastic2$experiment <- rep(2, length(elastic2$stretch))
elastic <- rbind(elastic1, elastic2)
elastic$experiment <- factor(elastic$experiment)
```

We fit all the three possible linear models, respectively:

* The simple `distance ~ stretch` linear model on data, which will generate a single regression line.

* The model `distance ~ stretch + experiment`, which will generate a line for each experiment varying only the intercept.

* The model `distance ~ stretch * experiment` in which the interaction factor between `stretch` and `experiment` is taken into account, and the two regression lines are no longer parallel.

```{r echo=TRUE, message=FALSE, warning=FALSE}
lm1 <- lm(distance ~ stretch, data = elastic)
lm2 <- lm(distance ~ stretch + experiment, data = elastic)
lm3 <- lm(distance ~ stretch * experiment, data = elastic)
```

We visualize the diagnostic plots for our models:

```{r echo=TRUE, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
plot(lm1)
plot(lm2)
plot(lm3)
```

We can see from those that the observation `7` may be problematic because of its high residuals, and the Residual vs Leverage plot `lm3` sees it as an influential outlier. We proceed to a preliminary ANOVA test to test model significance:

```{r echo=TRUE, message=FALSE, warning=FALSE}
anova(lm1, lm2, lm3)
```

We see there is only a low degree of evidence in favor of the model where we vary the intercept based on the experiment. We now try to get rid of the problematic observation mentioned before and repeat the same procedure:

```{r echo=TRUE, message=FALSE, warning=FALSE}
elasticfix = elastic[-7,]
lm1 <- lm(distance ~ stretch, data = elasticfix)
lm2 <- lm(distance ~ stretch + experiment, data = elasticfix)
lm3 <- lm(distance ~ stretch * experiment, data = elasticfix)
anova(lm1, lm2, lm3)
```

We see that after getting rid of `7` we have no significant evidence of a difference between the regression lines of the models.

# Core Statistics

## Exercise 3.2

**Rewrite the following to eliminate the loops, first using `apply` and then using `rowSums`:**

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
X <- matrix(runif(100000),1000,100); z1 <- rep(0,1000)

ptm <- proc.time()
for (i in 1:1000) {
  for (j in 1:100) z1[i] <- z1[i] + X[i,j]
}
proc.time() - ptm
```

**Confirm that all three versions give the same answers, but that your rewrites are much faster than the original. (`system.time` is a useful function.)**

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
X <- matrix(runif(100000),1000,100); z2 <- rep(0,1000)
system.time(z2 <- apply(X, 1, sum))
all.equal(z1, z2)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
X <- matrix(runif(100000),1000,100); z3 <- rep(0,1000)
system.time(z3 <- rowSums(X))
all.equal(z1, z2, z3)
```

The `rowSums` function is clearly the most optimized one for this task, with `apply` being a close second.

## Exercise 4.4

**Suppose that you have $n$ independent measurements of times between major aircraft disasters, $t_i$, and believe that the probability density function for the $t_i$’s is of the form: $f(t) = ke^{−λt^2},\;\;t≥0$ where $λ$ and $k$ are the same for all $i$.**

**(a) By considering the normal p.d.f., show that $k=\sqrt{4λ/π}$.**

We consider the normal p.d.f. centered in 0 ($\mu = 0$) with only positive values (since $t = x - \mu = x \geq 0$, so the normal p.d.f. must be multiplied by 2) to match the p.d.f. specified for the $t_i$'s:

$$f(t) = \frac{2}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{(x - \mu)^2}{2\sigma^2}\} = ke^{-\lambda t^2}$$

where $k = \frac{2}{\sqrt{2\pi\sigma^2}}$, $t = (x - \mu)$ and $\lambda = \frac{1}{2\sigma^2}$. The exercise want us to prove that $k = \sqrt{4λ/π}$, which is true since

$$k = \frac{2}{\sqrt{2\pi\sigma^2}} = \sqrt{\frac{4}{2\pi\sigma^2}} = \sqrt{4 \frac{1}{2\sigma^2}\frac{1}{\pi}} = \sqrt{4\lambda/\pi}$$

We assume that the exercise is wrong, and use the relation $k = \sqrt{\lambda/\pi}$

**(b) Obtain a maximum likelihood estimator for $λ$.**

First of all, we can compute the likelihood function for our distribution:

$$L(t) = \prod_{i = 0}^nke^{-\lambda t^2} = k^n \prod_{i = 0}^n e^{-\lambda t_i^2}$$

After that, we may apply the logarithm on $L$ to obtain the log likelihood of our distribution, that will make the estimation of our MLE for $\lambda$ easier (log transformation is invariant in maxima and minima of the original function):

$$\ell(t) = n \log(k) - \lambda \sum_{i = 0}^n t_i^2 = n \log(\sqrt{4\lambda/\pi}) - \lambda \sum_{i = 0}^n t_i^2$$

Finally, in order to obtain the maximum likelihood estimator for $\lambda$ we take the derivative of the log likelihood and set it to 0:

$$\frac{d\ell}{d\lambda} = \frac{n}{2\lambda} - \sum_{i = 0}^n t_i^2 = 0$$
$$\frac{2\lambda}{n} = \frac{1}{\sum_{i = 0}^n t_i^2}$$

Finally, by simplifying the previous expression we obtain the value of $\lambda$'s MLE:

$$\lambda = \frac{n}{2 \sum_{i = 0}^n t_i^2}$$

**(c) Given observations of $T_i$ (in days) of: 243, 14, 121, 63, 45, 407 and 34 use a generalised likelihood ratio test to test $H_0: λ = 10^{−4}$ against the alternative of no restriction on $λ$ at the 5% significance level. Note that if $V∼χ^2_1$ then $\text{Pr}[V≤3.841] = 0.95$**

```{r echo=TRUE,  message=FALSE, warning=FALSE}
 loglik <- function(data, param){
   length(data) * log(sqrt(4 * param / pi)) - param * sum(data^2)
 }

data <- c(243, 14, 121, 63, 45, 407, 34)

mle <- length(data) / (2 * sum(data^2))
print(paste("MLE for lambda given data:", mle))

l_0 <- loglik(data, 10^-4)
l_a <- loglik(data, mle) # When there is no restriction on lambda, we use the MLE to compute the log likelihood.

lambda_lrt <- -2 * (l_0 - l_a)

p_lrt <- pchisq(lambda_lrt, df=1, lower.tail = FALSE)
print(paste("Likelihood ratio test p-value:", p_lrt))
```

Since the p-value for the likelihood ratio test with one degree of freedom is very low, we reject the null hypothesis, which is coherent given the MLE for our parameter $\lambda$ which is different from the one taken as null hypothesis.

# Bayesian Computation

## Exericise 3.3

**Learning about the upper bound of a discrete uniform density**

**Suppose one takes independent observations $y_1, ..., y_n$ from a uniform distribution on the set ${1,2, ..., N}$, where the upper bound $N$ is unknown. Suppose one places a uniform prior for $N$ on the values $1, ..., B$, where $B$ is known. Then the posterior probabilities for $N$ are given by** 

$$g(N|y)∝\frac{1}{N^n},\;\;y_{(n)}≤N≤B$$

**where $y_{(n)}$ is the maximum observation. To illustrate this situation, suppose a tourist is waiting for a taxi in a city. During this waiting time, she observes five taxis with the numbers 43, 24, 100, 35, and 85. She assumes that taxis in this city are numbered from 1 to $N$, she is equally likely to observe any numbered taxi at a given time, and observations are independent. She also knows that there cannot be more than 200 taxis in the city.**

**a) Use R to compute the posterior probabilities of $N$ on a grid of values.**

```{r echo=TRUE, message=FALSE, warning=FALSE}
y <- c(43, 24, 100, 35, 85)
B <- 200
N <- 1:B

# There can't be less than max(Y) = 100 taxis, posterior between 0 and 99 = 0%
posterior <- function(N, y) {
  return(ifelse( max(y) > N, 0, 1 / N^length(y)))
}

prob <- sapply(N, posterior, y = y)
prob <- prob / sum(prob) # Scale by sum to obtain final probabilities

df <- data.frame(N, prob)

ggplot(df,aes(N,prob)) + 
  geom_col(colour = "blue") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(limits = c(100, 200)) +
  labs(title = "Posterior distribution for N",
       x = "N",
       y = "Posterior probability")
```

**b) Compute the posterior mean and posterior standard deviation of $N$.**

```{r echo=TRUE, message=FALSE, warning=FALSE}
mean <- sum(N * prob)
sd <- sqrt(sum(prob * (N - mean)^2))
print(paste("Posterior mean:", mean))
print(paste("Posterior standard deviation:", sd))
```

**c) Find the probability that there are more than 150 taxis in the city.**

```{r echo=TRUE, message=FALSE, warning=FALSE}
print(paste("Probability of having more than 150 taxis:", sum(df[df$N > 150,]$prob)))
```
