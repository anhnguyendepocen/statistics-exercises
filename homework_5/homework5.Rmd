---
title: "Homework 5 - Sarti"
author: "Gabriele Sarti"
date: "June 23, 2019"
output:
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
library(DAAG); library(ggplot2)
```

# Data Analysis and Graphics Using R

## Exercise 4.21

**Suppose the mean reaction time to a particular stimulus has been estimated in several previous studies, and it appears to be approximately normally distributed with mean $0.35$ seconds with standard deviation $0.1$ seconds. On the basis of 10 new observations, the mean reaction time is estimated to be $0.45$ seconds with an estimated standard deviation of $0.15$ seconds. Based onthe sample information, what is the maximum likelihood estimator for the true mean reaction time? What is the Bayes’ estimate of the mean reaction time?**

Since the data are approximately normally distributed, the maximum likelihood estimators for the true mean reaction time are equal to the sample mean, $0.45$, and the unadjusted sample variance $0.15^2 = 0.0225$.

In order to obtain the Bayes' estimate of the mean reaction time we use the formulas for the posterior density of the mean presented in chapter 4.8.2, which is normal with mean

$$\frac{n \bar y + \mu_0 \sigma^2 / \sigma_0^2}{n + \sigma^2 / \sigma_0^2}$$
and variance

$$\frac{\sigma^2}{n + \sigma^2 / \sigma_0^2}$$

We substitute the variables in the formula with the values given by the assignment:

* $n$ is the added sample size, $10$

* $\mu_0$ is the estimated mean of the original sample, $0.35$

* $\sigma_0$ is the standard deviation of the original sample, $0.1$

* $\bar y$ is the new sample mean, $0.45$

* We don't have the true standard deviation $\sigma$, but we can estimate it using the new sample standard deviation, $0.15$

We proceed to compute Bayes' estimates for both the mean and the variance of the distribution by computing the posterior mean and variance:

```{r echo=TRUE, message=FALSE, warning=FALSE}
n = 10
mu0 = 0.35
s0 = 0.1
y = 0.45
s = 0.15

print(paste("Bayes mean:", (n * y + mu0 * s^2 / s0^2)/(n + s^2 / s0^2)))
print(paste("Bayes variance:", (s^2)/(n + s^2 / s0^2)))
```

## Exercise 7.3

**Use the method of Section 7.3 to compare, formally, the regression lines for the two data frames `elastic1` and `elastic2` from Exercise 1 in Chapter 5.**

We start by creating single dataframe from `elastic1` and `elastic2`:

```{r echo=TRUE, message=FALSE, warning=FALSE}
elastic1$experiment <- rep(1, length(elastic1$stretch))
elastic2$experiment <- rep(2, length(elastic2$stretch))
elastic <- rbind(elastic1, elastic2)
elastic$experiment <- factor(elastic$experiment)
```

We fit all the three possible linear models, respectively:

* The simple `distance ~ stretch` linear model on data, which will generate a single regression line.

* The model `distance ~ stretch + experiment`, which will generate a line for each experiment varying only the intercept.

* The model `distance ~ stretch * experiment` in which the interaction factor between `stretch` and `experiment` is taken into account, and the two regression lines are no longer parallel.

```{r echo=TRUE, message=FALSE, warning=FALSE}
lm1 <- lm(distance ~ stretch, data = elastic)
lm2 <- lm(distance ~ stretch + experiment, data = elastic)
lm3 <- lm(distance ~ stretch * experiment, data = elastic)
```

We visualize the diagnostic plots for our models:

```{r echo=TRUE, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
plot(lm1)
plot(lm2)
plot(lm3)
```

We can see from those that the observation `7` may be problematic because of its high residuals, and the Residual vs Leverage plot `lm3` sees it as an influential outlier. We proceed to a preliminary ANOVA test to test model significance:

```{r echo=TRUE, message=FALSE, warning=FALSE}
anova(lm1, lm2, lm3)
```

We see there is only a low degree of evidence in favor of the model where we vary the intercept based on the experiment. We now try to get rid of the problematic observation mentioned before and repeat the same procedure:

```{r echo=TRUE, message=FALSE, warning=FALSE}
elasticfix = elastic[-7,]
lm1 <- lm(distance ~ stretch, data = elasticfix)
lm2 <- lm(distance ~ stretch + experiment, data = elasticfix)
lm3 <- lm(distance ~ stretch * experiment, data = elasticfix)
anova(lm1, lm2, lm3)
```

We see that after getting rid of `7` we have no significant evidence of a difference between the regression lines of the models.

# Core Statistics

## Exercise 3.2

**Rewrite the following to eliminate the loops, first using `apply` and then using `rowSums`:**

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
X <- matrix(runif(100000),1000,100); z1 <- rep(0,1000)

ptm <- proc.time()
for (i in 1:1000) {
  for (j in 1:100) z1[i] <- z1[i] + X[i,j]
}
proc.time() - ptm
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
X <- matrix(runif(100000),1000,100); z2 <- rep(0,1000)
system.time(z2 <- apply(X, 1, sum))
all.equal(z1, z2)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(42)
X <- matrix(runif(100000),1000,100); z3 <- rep(0,1000)
system.time(z3 <- rowSums(X))
all.equal(z1, z2, z3)
```

The `rowSums` function is clearly the most optimized one for this task, with `apply` being a close second.

**Confirm that all three versions give the same answers, but that your rewrites are much faster than the original. (`system.time` is a useful function.)**

## Exercise 4.6

**?**

# Bayesian Computation

## Exericise 3.3

**Learning about the upper bound of a discrete uniform density**

**Suppose one takes independent observations $y_1, ..., y_n$ from a uniform distribution on the set ${1,2, ..., N}$, where the upper bound $N$ is unknown. Suppose one places a uniform prior for $N$ on the values $1, ..., B$, where $B$ is known. Then the posterior probabilities for $N$ are given by** 

$$g(N|y)∝\frac{1}{N^n},\;\;y_{(n)}≤N≤B$$

**where $y_{(n)}$ is the maximum observation. To illustrate this situation, suppose a tourist is waiting for a taxi in a city. During this waiting time, she observes five taxis with the numbers 43, 24, 100, 35, and 85. She assumes that taxis in this city are numbered from 1 to $N$, she is equally likely to observe any numbered taxi at a given time, and observations are independent. She also knows that there cannot be more than 200 taxis in the city.**

**a) Use R to compute the posterior probabilities of $N$ on a grid of values.**

```{r echo=TRUE, message=FALSE, warning=FALSE}
y <- c(43, 24, 100, 35, 85)
B <- 200
N <- 1:B

# There can't be less than max(Y) = 100 taxis, posterior between 0 and 99 = 0%
posterior <- function(N, y) {
  return(ifelse( max(y) > N, 0, 1 / N^length(y)))
}

prob <- sapply(N, posterior, y = y)
prob <- prob / sum(prob) # Scale by sum to obtain final probabilities

df <- data.frame(N, prob)

ggplot(df,aes(N,prob)) + 
  geom_col(colour = "blue") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(limits = c(100, 200)) +
  labs(title = "Posterior distribution for N",
       x = "N",
       y = "Posterior probability")
```

**b) Compute the posterior mean and posterior standard deviation of $N$.**

```{r echo=TRUE, message=FALSE, warning=FALSE}
mean <- sum(N * prob)
sd <- sqrt(sum(prob * (N - mean)^2))
print(paste("Posterior mean:", mean))
print(paste("Posterior standard deviation:", sd))
```

**c) Find the probability that there are more than 150 taxis in the city.**

```{r echo=TRUE, message=FALSE, warning=FALSE}
print(paste("Probability of having more than 150 taxis:", sum(df[df$N > 150,]$prob)))
```
